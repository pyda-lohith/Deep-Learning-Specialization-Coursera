#################################### Week1 #############################################################################################
------------------------------------ Setting up Machine learning application------------------------

------Bias/ Variance
If we fit a straight line to that may be logistic regression fit. It is underfitting.
If we use a Neural network with hidden layers it will fit to the data exactly. It is overfitting.
In higher dimensional data we cannot plot the data and visualize the boundry instead we look at different metrics to evaluate bias and variance. 
I.e training set error and dev set error. In general all these errors are calculated based on human error (i.e 0%).

------ Basic recipe of ML
Higher bias:	Bigger network (More hidden layers)
				Train longer
				
Once you reduce bias to accpetable amount we ask do you have variance problem. 

High variance:	Get more data	
				Regularization
				
Reduce bias and reduce variance without hurting other things. In Deep leraning we will bother much less about the bias-variance tradeoff. 
				
------------------------------------  Regularizing neural network ------------------------------------

----- Regularization
Let develop regularization using logistic regression. Lamda--> regularization parameter. 
L2 norm or euclidian norm. Why do we regularize just the parameter W. Why don't we add paramter b as well. It is most common type of regularization.
L1 regularization instead of adding L2 norm you add the expression.
If we use L1 regularization we need up with w will be sparse. This can help compressing the model.   
Lamda will be set using development set or hold-out cross validation. We will represent lambd. 
For this reason we call L2 regularization as wedge decay. 


----- why regularization reduce over-fitting
Why is it shrinking the L2 Norm or Forbenous norm will reduce over-fitting. 
We will set Lamda to be big and then intialize W[L] = 0 (almost) for lot of hidden units because zeroing out impact of lot of hidden units. Then this 
would become much simplied smaller neural network infact almost logistic regression unit. 

If Lamda large  W[L] would decrease then z[L] small range of values. Then the activation function tanh will be relatively linear. 

------ Dropout regularization
We toss-up the coin and set some propability 0.5% and remove the node. 
d3- > dropout of layer3
a3 -> activations of layer3
In order to not reduce the expected value of Z[4] we will divide by 0.8.  This will pump back or correct the 20% that you need for Z[4]i.e a[3]/0.8 (keep prob). 
This will keep the expected value of a[3] by same. This is called as inverted dropout. 

------- Understanding Dropout:
We can keep different propability fro each layers. 

------- Other regularization methods:
Before iteration W would be almost 0. After iterations W would be large. In the mid-size we will stop early. 
Orthoganilization we call for 2 steps. Both are independent of each of each other. 
One downside of early stopping is that it couples both the tasks and we can no longer work on these steps independently. 

------------------------------------ Setting up optimization probelm --------------------------------------

------ Normalizing inputs:
Normalizing is 2 step process. i.e subtract mean and normalize variance. 
If we use unnormalized input features the cost function will look this elongated. If we plot the contours of this function we get elongated. Moreover
gradient descent will oscillate and takes lot of steps to converge.

------- vanishing gradient descent:
W[L] is >1 i,e 1.5. Say If L is large for deep neural network yhat will be very large i.e 1.5 times. 
W[L] is <1 i,e 0.5. The activation ends up decreasing exponentially.  
Which makes training takes more time. 
 
------- Weigh intialization of deep networks
large N we want smaller Wi. We have to set the variance of Wi to be equal to 1/n. If we use relu function then set to 2/n. 
other variants are there if we are using tanh activation function we use above formula. 

------ Gradient checking:
How to check these 2 vectors are approximate to each other. 
Check the euclidian distance between these 2 vectors i.e L2 norm of this. 

############################################################################################################################################################
------------------------------- Optimization algorithms -------------------------------------------------

------- Mini-Batch gardient descent
If m value is large calculating gradiebt descent fro all the records is time consuming. We will split to baby training sets called mini-batch.
Lets 1000 training set as mini-batch.
(i) -> ith training example
[l] -> index layer of neural network
{t} -> index into mini-batch
1epoch- > 1 single pass through the training set

------- Understanding Mini-Batch gardient descent
Stochastic gradient is too noisy but on average it takes to good direction sometime wrong direction as well .It won't converge it would oscillate and wonder
around the region of minimum but it won't hit the minimum. 
Disadvantage of stochastic is we will lose up almost the speed of vectorization. 


------- EXponentially weighted average
Beta -> controls exponential weighted average
If we want to calculate the moving average of the temperature this is what we can do. 
We think Vt as average over 1/(1-beta) days temperature.
When we increase beta the curve moves much slowly i.e green line(average over 50 days tempearature).

------- Understanding exponentially weighted average
2nd one is exponentially decaying function. we will make cross product of 2 functions.
All the coffecient of V100 add upto 1. 	In this example epsilon would be 0.1 so 1-epsilon= 0.9. In other words it takes about 10 days for the height of 
this to decay to 1/3rd of the peak. One advantage of exponentially weighted average is it takes low memory and keep on iterating over one value. 

------- Bias correction
Bias correction makes computation of averages more accurate. When we actually implement with 0.98 we will get purple curve which starts very low. How to fix
that. 

------ Gradient descent with momentum
We are computing the moving average of W that we are getting. 
This whole steps smooth out the process of gradient descent.  The average in vertical direction woul be zero. Where as in horizontol it would be big. 
Gradient descent with momentum endups eventually just much smaller oscillations in the vertical direction but much directed towards horizontol direction. 
Makes algorithm to damp out the oscillations in its path to minimum. These derivative terms think as acceleration to the bow that we are rolling down. 
Momentum terms representing velocity.  
In literature we wont consider (1-Beta) term. 

-------- RMS Prop:
If we implement gradient we end up with oscillations in vertical direction also makes progress in horizontol direction. In order to provide intuition 
lets say vertical axis b and horizontol axis W. Our aim is to speed up horizontol direction and slow down vertical direction. 
Sdb is large so 1/root of Sdbis small inorder to slow down the oscillation in vertical direction. 

------- Adam optimization algorithm
It combines gradient descnet with momentum with RMS prop.
Alpha -> needs to be tuned
beta1 -> 0.9 this is weighted average of dw momentum like term
bet2 -> 0.999 this is moving weighted average of dw**2 
epsilon -> 10-8
Adam ---> Adaptive moment estimation 

------- Learning rate decay
As we are using some fixed value of alpha the algorithm wont converge it just revolve aroung the global optimal minima. 
Slowly reduce alpha, As alpha get smaller the steps would be smaller so we end up oscillating in a tighter region around this minimum rather than wondering 
around this far away. I.e as learning approches convergence will take smaller steps having small learning rate.
decay-rate is one more hypermeter that we need to tune.

------- problem of local optima


###############################################################################################################################################

----------------------------------------------- Hyper parameter tuning

-------- Tuning process
If we use different hypermeter how do we select these values. In earleir generation of ML algorithm if we had 2 hypermeter it is common practise to sample
the points in grid. Whereas we will sample at random. We will normally use coarse to fine sampling scheme. In this scheme we will zoom in to the hyperparameter
region and sample more densly in that space. 


-------- use appropriate scale
Sampling random doesn't mean smapling uniformly at random over the range of valid values. Lets say we are trying to choose no of hidden units for a given layer 
L. Lets say good range of values is 50 to 100. Picking some values at random within this number line is pretty vesiable way to search for this particular 
hyperparameter. 
If we draw the number line from 0.0001 to 1 and sample the values uniformly random. Then 90% of values you sample will be between 0.1 and 1. Means we are 
using 90% of resource to search between 0.1 and 1 and remaining 10% search between 0.0001 and 0.1 that doesnt seem right. So we will search for hyperparameter 
on log scale. 
When Beta is close to 1 the sensitivity of the result would get changes even with small changes to Beta. If Beta goes from 0.9 to 0.9005. There is no big
change. But when Beta goes from 0.999 to 0.9995 this will have huge impact exactly what algorithm is doing. In general 1/(1-Beta) is very sensitive to data
when it is close to 1. 

-------- Tuning in practise
Way to choose panda approach and cavier depends on computational resources we have. If we have enough computers try cavier approach. 

------------------------------------------------- Batch Normalization

--------- Normalizing activations
Batch normalization makes the hyperparameter search more easier and makes Neural networks more robust. Makes easy to train more deep NN. In logistic regression 
Normalizing input features will speed up learning.  Where as in NN we have inputs, activations etc. If we want to train the parameters say w3, b3 then it 
won't be nice if we normalize the mean and variance of a[2] to make it more effecient. For any hidden layer can we normalize the values of a[2] so as to 
train w[3], b[3] fast.
In practise z[2] will be done much more often. Mean 0 and variance 1 we will try to do normally. But we don't want to make mean 0 and variance 1 for hidden
units. Here Gamma and Beta are learnible parametrs of the model. Gamma and Beta set the value of Ztilda to be whatever we want. 

--------- Fitting batch norm
This Beta is nothing to do with Beta i.e momentum. 
To update the parameters Beta and Gamma we can use gradient descent. Or we can use adam, rms prop or momentum. In practise we use tf.nn.batch_normalization. 
In practise we apply batch-norm with minibatches. 
In using we can eliminate b parameter because batch norm step compute the mean of z[l] and subtract out the mean. So adding any constant to all of these 
examples in mini-batch doesnt add anything. 

--------- Why does batch-norm work
If the distribution of X changes then we need to retrain the model -> covariate shift
As w[1] b[1] and w[2] b[2] changes then a1[2] a2[2] etc also changes. From perspective of third hidden layer as These hidden unit values changing all the time
suffering from problem of covriate shift.  Batch normalization reduces the variation in weights. Even if the exact values of z1[2] and z2[2] change
their mean and variance remains same. 

--------- Batch norm at test time
m -> denotes no of examples in mini batch
Mue and sigma are calculated on the entire mini-batch. But at test time even won't have mini-batch like 64, 128 examples at same time.

--------------------------------------------------- Multi-class classification

-------- Training a softmax classifier
hardmax function looks for element with biggest value and keeps 1. Whereas softmax is gentle mapping where as Z to these propabilities. 

---------------------------------------------------- Programming Frameworks

------- Tensorflow
session.run(init) ---> will intialiaze global variables 
for tensor flow to evaluate a variable we use sess.run()

tensorflow placeholder tells that x is provide values later. 

these 3 lines if code is somewhat idiomatic. 
This line of code allows tensorflow to draw a computational graph. While writing forward propagation equation tensorflow has already builtin function 
backward propagation. 