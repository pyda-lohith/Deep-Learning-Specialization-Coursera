Week2-----

Binary classification:
nx or n--> represents the dimensions of input feature X i.e 12288(64*64*3)
A single training example is represented by pair (X,y), where X is nx dimesional feature vector and y is 0 or1
Training set contains m training examples 
To put all of the training examples into compact notation we define matrix X as defined by taking examples X(1),y(1) etc and stacking them in columns.
This matrix has m columns i.e no of training exampless and rows is nx. Notice that in other courses we might see might see matrix X by stacking up training 
examples in rows,but we wont use here. We would use the above for NN which is easier.
Y is 1*m matrix

Logistic regression:
X is nx dimensional vector given that parameters of logistic regression w is nx, b is 1
the output should generate yhat i.e yhat= traspose(w)+b  
Infact we will use the same in linear regression, but not good for classification beacuase yhat should be 0 or 1. so apply sigmoid function
We treat w and b has separate parameters.

Logistic regression cost function:
To train the parametrs w and b we need to find cost function.
x(i) ith trainig example
loss function one half of (yhat-y)*2 error people usually don't do  because optimization function is nonconvex and it is with multiple local opyimal minimum.
Loss function measures how good yhat is. As squared error might not work well for logistic regression we define which is similar role as squared error that 
give us optimization function which gives convex. 
Loss funcction is for similar training example.
cost function for entire training set, it is going to be average of loss function.  


Graddient descent:
we want to find w,b that minimize j(w,b) as small as possible. Hieght of the surface is j(w,b) as convex function single big bowl.
Intialize w,b to some intial value denoted by red dot. Take one stepest step down hill direction one step of gradient descent. repeat to converge local optima.
update w for each iteration

Computation graph and derivates:
In forward pass compute the output of neural network.In backward pass to compute derivatives or gradients. 
To compute derivates we move from right to left computation. 
When calculating backward propagation we hear about final output variable is J. They ask us to calculate derivative of final output variable with respect to 
some intermediate variable. call as dJdVar or dVar


Logistic regression gradient descent:
using computation graphs. Recap logistic regression is represented as follows. 
Let say we have only two features x1 and x2. we input w1 and w2. 
dz= a-y
dw1= x1.dz  dw2= x2.dz db=dz


Gradient descent on m examples:
In the last chapter we dealt with one training example. (i) means every training example. 
j(w,b) is average of loss function. Like to calculate derivative of j(w,b) with dw1, it would be same like average and is represented ass dw1 in previous 
training example and average them dw1(i).
Let wrap up all of them using for loop
j= j/m as it is average of all training example

Having completed all these calculation to implement one step of gradient descent.
w1= w1-alpha*dw1
2 for loops we are implementing for i=1 to m and dw1(depends on no of features)
to get rid of these explicit for loops we use vectorization


Python and vectorization:
w and x are nx dimensional vectors.

Vectorizing logistic regression:
we know X has m training inputs stacked together. i.e (nx,m). will show how to compute z1,z2,z3. Consider [b b b ...] as 1*m vector.
W transpose is row vector.
As Z is 1*m vector  A will also be 1*m vector

Vecctorizing logistic regression gradient descent:
dZ= A-Y
previous we got rid of one for loop already. But dw loop was still there. similarly for db=0
one best thing is summarizing dz to get db.
to get dw we need to trasnpose dz since it is 1*m to m*1, X is nx*m  we will get n*1 vector

If We want to implement multiple iterations of gradient descent we might still need for loop over no of iterations.


-------------------------------------------------------------------------------------------------------------------------------------------------------

Week3:--------

Neural Network Representation:
Values of input features is a[0] a stands for activation of input layer 
Hidden layer inturn generate activation and is a[1] and is 4*1 matrix
output layer is a[2] and is real number
Whole network is 2 layer neural network and the reason is we don't count the input layer. 
further hidden layerf and output layers have parametrs associated withit i.e w,b. for a[1] has w[1] and b[1].
w[1] is 4*3 i.e 4 represents no of hidden units in the layer and 3 represnts no of input features.
b[1] is 4*1 i.e 4 represnts no of hidden units
w[2] is 1*4 i.e 1 hidden unit and 4 input features(i.e hidden layer inputs)

Computing a neural network output:
A single circle represents 2 steps i.e z and a. Lets see one hidden layer
a[l]i  represents l is hidden layer and i is the node in that hidden layer.
Doing all these in for loop is very tedious. so take 4 equations and vetorize. Take w's and stack them into matrix then we have w[1]1T so that column vector
transpose gives row vector. so it is 4*3 matrix and X is 3*1 matrix.

Vectorizing across multiple example:
Suppose we have m training examples 
a[2](i)  --> i represnts training example and 2 hidden layer2. To calculate we use for loops for all the 4 formulas.
We define matrix X with training examples i.e nx,m matrix. In order to implement vectorizzed notation of for loop.
We went from lower case x to capital case X by stacking lower case x in columns. Same for Z[1] and A[1]. These matrix Z,A horizantally we are going to index 
across m training examples and Vertical nodes coressponds to different layers in neural network (i.e Hidden units number).

Explanation for vectorized implementattion:
Make b term 0. 
W[1] is some no of rows, w[1]X[1] give some column vector. Likewiese W[1]X[2] Gives column vector.Now if we consider training set X.
Same Z[1] also stacked up in columns.

Activation Function:
What activation function to use in hidden layers as well as output unit of NN.
g could be a non-linear function that may not be sigmoid function. An function that perform better than sigmoidis tanh function i.e -1 to +1. because the mean of the
activation that come out of the hidden layer is closer to 0. We might center our data to 0,, make learning for 2nd layer easier.
Output we sigmoid activation function. The activation is different for different layers so we use g[1], g[2]. Now one of the downsides of sigmoid and tanh 
function is that if z is very large or small then gradient or slope of the function is small close to zero and can slow down gradient descent. So one of the popular
we use relu function, the derivative is 1 as long as z is positive and 0 as long as z is negative.So NN would often learn faster. a= max(0,z)
There is other version of relu i.e leaky relu function, there would be slight slope for negative values. (0.01z,z) 

Non-linear activation function:
Why dont we set a[1]= z[1] i.e g(z)=Z  linear activation function
This function computes yhat as linear function of input feature X.NN is outputing a linear function Wdash+bdash. This model is like standard logistic rgerssion without
any hidden layer.We use linear activation function in output layer If y is real number and we want to predict housing prices upto expense, but we use relu in layer1.

Derivatives of activation function:
sigmoid activation-
When Z is very large the slope is close to 0.
Z is 0 the g(z) 1/z
shorthand gdash(z) of gprime(z)
gdash(z)= a(1-a)
tanh activation-
gdash(z)= 1-a**2

Gradient descent for neural network:
nx alternatively n[0] input features, n[1] hidden units and n[2] output units
Cost function- doing binary classification
Gradient- When training NN we need to intialize parametrs randomly rather than 0
We repeat till m training examples. db[2] is (n[2],) to avoid we use keepdim so that (n[2],1)
so far what we have done is for logistic regression 
if we run backward propagation for dz[1] we get this
g[1]prime is derivative of the activation function we use for hidden layer. Output layer is sigmoid activation function dz[2].
It is elementwise product.

Backward propagation intuition:
We know dz[2] and da[2] simple for ligistic regression. Also we have for dw[2] and db[2]. In logistic regression we have dw=dz*x where a[1] plays role of x.
w is row vector in logistic regression for single output.
In General computation of da[1] and dz[1] are collapsed into 1 step.
In Vectorized implementation we took z[1] and stacked up to columns. 

Random Intialization:
For logistic --> ok to intialize weights with 0
For neural --> it won't work 
Intializing bias term to 0 is ok
For any example we give it we have a[1]1= a[2]2 
Then w[2]= [0 0] and is completly symmetric. After many iteration both hidden units would be computing the same function.So we intialize paramters randomly.
Prefer to intialize weights with smaller values so 0.01 we use instead of 100,1000 etc. We are using sigmoid or tanh function If weights are too large and 
compute the activation values If w is big z will be big it will end up in flat parts i.e slope of gradient is very small and learning will be very slow.

--------------------------------------------------------------------------------------------------------------------------------------------------------------
Week4----

Deep l-layer Neural network
Logistic- is shallow model also 1 layer NN where as the other is deep model
L= no of layer
n[l]= no of units in layer L
a[l]= activation in layer L

Forward propagation in deep network:
We are z and a and stacking them up to m training exaamples to form Z.To summarize out notation replace lower case z and a with upper case Z. Which gives
vectorized version of forward propagation. 
It looks like there is for loop for l i.e from 1 to 4. Then we have to computate activation for layer1, layer2 etc.

Getting your matrix dimensions right:
Let thin k about dimesions of Z and X.Z[1] is no of activations for first hidden layer.
z[1] is 3*1 dimesion i.e n[1]*1 dimesional vector. X is 2*1 matrix and n[0]*1 dimesion. matrix W[1] should be something that make 3*1 matrix. so W must be
3*2 matrix to satify the above condition i.e n[1]*n[0] matrix. In general w[l]= (n[l],n[l-1]) matrix.
In general b[l] is (n[l],1). Z and a should have same dimensions in the network.
As we are stacking m training examples z[1] 

why deep representation:
Face recognition deep neural network would be doing. First layer is feature detector or edge detector.Lets a 20 hidden units on trhis image. when trying
to visulaize by these sqaure boxes. This box figure out horizantal edge, next box figure out vertical edge ib image. Propably first layer of NN figure out edge 
in NN think for sometime. Now these figured out edge are grouped pixels together to form edges.Group edges together to form parts of face like nose, eyes etc.
And then finally by putting together different parts of faces we can detect different types of faces.
Edges detector are looking for small areas in an image. Face detector look for larger area in the image. This types of simple to complex hirerchial 
representation is used in other types of data than images.
Speech recognition- first level detect low level audio wave form features. These low level waveforms are composed together to form basic units of sound 
called phonemes. We can compose phonemes together to form words etc.
In simpler human brain also think of simpler things first like edges etc through eyes and compose together to form face.
The other piece of intuition why deep NN is follows. y is the parity of all these input bits. So to compute XOR, the definite left network will be on the 
order of logN. We will just have XOR tree. You don't need that many gates in order to compute the XOR. But now if are not allowed to use a NN with multiple 
hidden layers. In this case order log and hidden layers. If ypou are forced to compute this function with just one hidden layer. In order to compute the
parity or XOr of this hidden layer.This hidden layer must be exponentially large. You need to enumerate our 2 to the N possible configurations of the input bits
that result in either 1 or 0. So this gives a sense that there are mathematical functions, that are much easier to compute with deep network than with shallow
network.

Building blocks of deep neural network:
Lets take one layer and look for computations for now. In turns out that for later use will be useful to cache the value of z[l]. Storing the values of Z[l]
will be useful for backward propagation step.
if we implement these two functions then basic computation of NN will be as follows.
da[0] is not necessary as it is change in inputs given which is constant.
In general we will cache z[l], w[l], b[l].

Forward and backward propagation:
One last detail that we talk is forward recursion we will intialize with the input data X. How about backward recursion?
It turns out that da[l] when using logistic regression. 
for dA[l] it should be sum of all training examples.

Parameters Vs Hyperparameters:
In fact deep learning has lot of different hyper paramters like momentum, mini batch size, various forms of regularization etc.
if we are not sure about learning rate alpha we will plot the graph againist cost J and no of iterations.

 